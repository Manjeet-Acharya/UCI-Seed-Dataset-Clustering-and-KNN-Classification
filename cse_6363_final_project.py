# -*- coding: utf-8 -*-
"""CSE_6363_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ployeKKs6Sd7fDg0HeZBerb4IQ7Ne5KV
"""

# Mount the Goole drive to Colab

from google.colab import drive
drive.mount('/content/drive')

# importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
from pandas.core import describe
import random
import math
from scipy.cluster import hierarchy
import matplotlib.pyplot as plt
from pandas.core.common import random_state

# Read .txt files

path = '/content/drive/MyDrive/final_project_data/seeds_dataset.txt'
# reading the .xls file into a dataframe
seed_data = pd.read_csv(path, sep = '\s{1,}', 
                              engine = 'python', header=None)

"""**Splitting the testing data before clustering**"""

# getting the name of the last column that has class label
last_column = list(seed_data.columns)[-1]

# taking the random 10 rows of each class labels as testing data
ten_label_1_data = seed_data.loc[(seed_data[last_column]== 1)].sample(10, random_state=1)
ten_label_2_data = seed_data.loc[(seed_data[last_column]== 2)].sample(10, random_state=1)
ten_label_3_data = seed_data.loc[(seed_data[last_column]== 3)].sample(10, random_state=1)
testing_df = pd.concat([ten_label_1_data, ten_label_2_data , ten_label_3_data ])

# creating a list of list from testing dataframe and class labels of each data point
testing_data_actual_labels = testing_df[testing_df.columns[-1]]
del testing_df[testing_df.columns[-1]]
testing_data = testing_df.values.tolist()

# Taking the remaining data points as the data for clustering
clustering_data = seed_data.drop(testing_df.index).reset_index(drop = True)

# dropping the last column of the dataframe.
# this column contains the class of the sample
labels = clustering_data[clustering_data.columns[-1]]
del clustering_data[clustering_data.columns[-1]]

clustering_data

testing_data

def hierarchical_clustering(clustering_data, linkage, n_clusters):
  original_data = clustering_data.copy(deep = True)
  # defining a function to normalize the data
  def data_normalization(clustering_data_df):
    # creating a description dataframe to get the min and 
    # max value for each column(feature)
    description = clustering_data_df.describe().transpose()
    mean_norm = (clustering_data_df - description['min'])/(
          description['max'] - description['min'])
    # returning a list of list with normalized data
    data_for_clustering = mean_norm.values.tolist()
    return (data_for_clustering)

  # defining a function that returns the eucledian distance
  # between two vectors
  def euclidean_distance(vector_a, vector_b):
    p_minus_q_squared = 0
    if len(vector_a) != len(vector_b):
      return('Error! Vectors with different dimensions')
    else:
      for i in range(len(vector_a)):
        p_minus_q_squared += (vector_a[i]-vector_b[i])**2
      eucledian_distance = math.sqrt(p_minus_q_squared)
    return(eucledian_distance)

  # normalizing the data by calling the above function
  data = data_normalization(clustering_data)

  # creating a distance matrix for similarity measurement
  index_list = []
  column_list = []
  for i in range(len(data)):
    point = 'p'+ str(i)
    index_list.append(point)
    column_list.append(point)
  distance_matrix_dataframe = pd.DataFrame(columns = column_list, 
                                         index= index_list)
  
  # filling the above distance matrix by calling the distance
  # calculation function that was created above
  for p in range(len(data)):
    row_index = 'p' + str(p)
    for q in range(len(data)):
      col_index = 'p' + str(q)
      if q <= p:
        continue
      else:
        dist = euclidean_distance(data[p], data[q])
      distance_matrix_dataframe.loc[row_index, [col_index]] = [dist]
      distance_matrix_dataframe.loc[col_index, [row_index]] = [dist]

  # defining a function that performs a single linkage
  def single_linkage(distance_dataframe, merge_p1, merge_p2, new_cluster_name):
    new_column = []
    for row in distance_dataframe.index:
      # skipping over the rows and columns that are being merged 
      if row == merge_p1 or row == merge_p2:
        continue
      else:
        # finding the minimum distance from a given point
        # to the two points that are being merged
        if distance_dataframe.loc[row, [merge_p1]][0] <= distance_dataframe.loc[row, [merge_p2]][0]:
        # creating a new list with the distance values for the merged points
          new_column.append(distance_dataframe.loc[row, [merge_p1]][0])
        else:
          new_column.append(distance_dataframe.loc[row, [merge_p2]][0])
    # dropping the rows and columns of the points that are being merged
    distance_dataframe.drop(merge_p1, axis=1,inplace=True)
    distance_dataframe.drop(merge_p2, axis=1,inplace=True)
    distance_dataframe.drop(merge_p1, axis=0,inplace=True)
    distance_dataframe.drop(merge_p2, axis=0,inplace=True)
    # filling the rows and columns of the new distance matrix
    # with the distance values from above
    distance_dataframe[new_cluster_name] = new_column
    new_column.append(np.nan)
    distance_dataframe.loc[new_cluster_name, :] = new_column
    # returning the new distance matrix
    return(distance_dataframe)

  # defining a function that performs a complete linkage
  def complete_linkage(distance_dataframe, merge_p1, merge_p2, new_cluster_name):
    new_column = []
    for row in distance_dataframe.index:
      # skipping over the rows and columns that are being merged 
      if row == merge_p1 or row == merge_p2:
        continue
      else:
         # finding the maximum distance from a given point
        # to the two points that are being merged
        if distance_dataframe.loc[row, 
                              [merge_p1]][0] > distance_dataframe.loc[row, 
                                                                [merge_p2]][0]:
          new_column.append(distance_dataframe.loc[row, [merge_p1]][0])
        else:
          new_column.append(distance_dataframe.loc[row, [merge_p2]][0])
    distance_dataframe.drop(merge_p1, axis=1,inplace=True)
    distance_dataframe.drop(merge_p2, axis=1,inplace=True)
    distance_dataframe.drop(merge_p1, axis=0,inplace=True)
    distance_dataframe.drop(merge_p2, axis=0,inplace=True)
    # filling the rows and columns of the new distance matrix
    # with the distance values from above
    distance_dataframe[new_cluster_name] = new_column
    new_column.append(np.nan)
    distance_dataframe.loc[new_cluster_name, :] = new_column
    # returning the updated distance matrix
    return(distance_dataframe)  

  # defining a function that returns n number of cluster
  def return_n_cluster(datfrm_indx, track_cluster_aggregation):
    clestring = {}
    count_clus = 0
    # iterating through the indexes of the clusters
    for indxs in datfrm_indx:
      if indxs in track_cluster_aggregation.keys():
        my_list = track_cluster_aggregation[indxs]
        # checking to see if the new cluster exists in the tracking dictionary
        # the tracking dictionary keeps track of which points belong which cluster
        while [i for i in my_list if i in track_cluster_aggregation.keys()]:
          a = [i for i in my_list if i in track_cluster_aggregation.keys()]
          for k in a:
            my_list.extend(track_cluster_aggregation[k])
            a.remove(k)
          my_list.remove(k)
        cluster_value = my_list
      else:
        # if the index only represents one point(the original sample), 
        # the value of that index is assigned to the cluster
        cluster_value = [indxs]
      clestring[count_clus] = cluster_value
      count_clus+=1
    # returning the cluster that tells which point belong to which cluster
    return(clestring)  

  # creating a copy of the original dataframe matrix
  dataframe = distance_matrix_dataframe.copy(deep= True)
  # creating empty list and dictionary for dendogram and cluster length
  dendogram = []
  cluster_length = {}
  track_cluster_aggregation = {}
  for p_i in dataframe.index:
    cluster_length[p_i] = 1
  number_of_clusters = len(dataframe)
  # iterating n number of times, where n is the length of the our dataset
  for i in range(len(dataframe)):
    # when we only have last two points to merge
    if len(dataframe) == 2:
      row_col_indx = []
      for ind in dataframe.index:
        row_col_indx.append(ind)
      # when there are only two points left to merge, the final 
      # cluster will contain all the items in the dataset 
      ith_cluster = [row_col_indx[0], row_col_indx[1], 
                   dataframe.loc[row_col_indx[0], [row_col_indx[1]]][0], 
                   len(distance_matrix_dataframe)]
      dendogram.append(ith_cluster)
      break
    else:
      # identifying the points that are going to merge
      # by looking at the highest similarity/least distance
      point_1, point_2 = np.unravel_index(
        np.nanargmin(dataframe.values), dataframe.shape)
      min_distance = dataframe.iloc[[point_1], [point_2]]
      row = min_distance.index[0]
      column = min_distance.columns[0]
      dist = min_distance.loc[row, [column]][0]
      no_of_items_in_cluster = ([v for k,v in cluster_length.items() if 
                             k == row][0] 
                            + 
                            [v for k,v in cluster_length.items() if 
                             k == column][0])
      # keeping track of points that are being merged, distance between
      # them and the number of items in that cluster that has just been created 
      # by merging the two points
      ith_cluster = [row, column, dist, no_of_items_in_cluster]
      dendogram.append(ith_cluster)
      new_cluster = 'p' + str(len(distance_matrix_dataframe)+i)
      cluster_length[new_cluster] = no_of_items_in_cluster
      # checking the linkage criterian and calling the respective function from above
      if linkage.lower() == 'single':
        dataframe = single_linkage(dataframe, row, column, 
                             new_cluster)
      if linkage.lower() == 'complete':
        dataframe = complete_linkage(dataframe, row, column, 
                             new_cluster)
      # keeping track of the number of clusters and 
      # keeping track of new point-name that represents 
      # the merged points from previous iteration
      number_of_clusters-=1
      track_cluster_aggregation[new_cluster] = [row, column]
      count = 0
      dataframe_indices = dataframe.index
      # calling the above function to get n number of clusters
      cluster_dict = return_n_cluster(dataframe_indices, track_cluster_aggregation)
      # Since we used 'p1', 'p2', etc to represent the data points in our algorithm, 
      # adding 'p' as prefix to our indices of the original data and selecting 
      # original n-dimentional data points for the respective clusters
      if number_of_clusters == n_clusters:
        return_cluster = {}
        original_dataframe = original_data.rename(
            index=lambda s: 'p'+ str(s))
        for key, value in cluster_dict.items():
          return_cluster[key] = []
          for items in value:
            return_cluster[key].append(
                original_dataframe.loc[items].values.tolist())
  # returning the clusters and the matrix for plotting dendogram
  return(return_cluster, dendogram)

# defining a function to calculate the accuracy of the clusters
def calculate_accuaracy(data, labels, cluster_dict):
  labeled_cluster = []
  overall_accuracy = 0
  # labeling each cluster by looking at highest occuring class label in
  # respective clusters
  for key, values in cluster_dict.items():
    if len(values) == 0:
      continue
    else:
      total_cluster_labels = []
      for p in range(len(values)):
        for q in range(len(data)):
          if values[p] == data[q]:
            label = labels[q]
            total_cluster_labels.append(label)
      most_freq_label = max(set(total_cluster_labels), 
                        key = total_cluster_labels.count)
      score = 0
      total_predicted = 0
      for p in range(len(values)):
        for q in range(len(data)):
          if values[p] == data[q]:
            total_predicted+=1
            if labels[q] == most_freq_label:
              score+=1
      accuracy_of_each_cluster = (score/total_predicted) * (len(values)/len(data))
      overall_accuracy += accuracy_of_each_cluster
  # calulating overall accuracy of the clusters based on the number of 
  # correct class label and number of items in those clusters
  #overall_accuracy = 0
  #for key, values in labeled_cluster.items():
    #predicted_labels = []
    #actual_labels = []
    #for value_i in values:
      #for i in range(len(data)):
        #if value_i == data[i]:
          #actual_labels.append(labels[i])
          #predicted_labels.append(key)

    #score = 0
    #for a in range(len(predicted_labels)):
      #if predicted_labels[a] == actual_labels[a]:
        #score+=1
    #accuracy_of_each_cluster = (score/len(predicted_labels)) * (len(values)/len(data))
    #overall_accuracy += accuracy_of_each_cluster
  # returning the overall accuracy of given clusters
  return(overall_accuracy)

"""**Single Linkage**"""

# checking to see if the above algorithm for checking accuracy is correct
# the accuracy for higher number of clusters should be higher

data_list = clustering_data.values.tolist()
clusters, dendogram = hierarchical_clustering(clustering_data, 
                        linkage = 'single', 
                        n_clusters = 178)
accuracy = calculate_accuaracy(data_list, labels, clusters)
print('When k = ', 178, 'accuracy is equal to:', accuracy)

clusters

data_list = clustering_data.values.tolist()
for i in range(1,10):
  clusters, dendogram = hierarchical_clustering(clustering_data, 
                        linkage = 'single', 
                        n_clusters = i+1)
  accuracy = calculate_accuaracy(data_list, labels, clusters)
  print('When k = ', i+1, 'accuracy is equal to:', accuracy)

clusters

for merge in dendogram:
  merge[0] = int(float(merge[0][1:]))
  merge[1] = int(float(merge[1][1:]))
dendogram_array = np.array(dendogram)

fig = plt.figure(figsize=(40,30))
dn = hierarchy.dendrogram(dendogram_array, 
                          above_threshold_color='y',
                          orientation='right')
plt.show

"""**Complete Linkage**"""

data_list = clustering_data.values.tolist()
for i in range(1,10):
  clusters, dendogram = hierarchical_clustering(clustering_data, 
                        linkage = 'complete', 
                        n_clusters = i+1)
  accuracy = calculate_accuaracy(data_list, labels, clusters)
  print('When k = ', i+1, 'accuracy is equal to:', accuracy)

complete_link_clusters, complete_link_dendogram = hierarchical_clustering(clustering_data, 
                        linkage = 'complete', 
                        n_clusters = 3)

for merge in dendogram:
  merge[0] = int(float(merge[0][1:]))
  merge[1] = int(float(merge[1][1:]))
dendogram_array = np.array(dendogram)

fig = plt.figure(figsize=(40,30))
dn = hierarchy.dendrogram(dendogram_array, 
                          above_threshold_color='y',
                          orientation='right')
plt.show

"""**Training the KNN Classifier**"""

def return_labeled_clusters(data, labels, cluster_dict):
  labeled_cluster = {}
  # labeling each cluster by looking at highest occuring class label in
  # respective clusters
  for key, values in cluster_dict.items():
    if len(values) == 0:
      continue
    else:
      total_cluster_labels = []
      for p in range(len(values)):
        for q in range(len(data)):
          if values[p] == data[q]:
            label = labels[q]
            total_cluster_labels.append(label)
      most_freq_label = max(set(total_cluster_labels), 
                        key = total_cluster_labels.count)
      labeled_cluster[most_freq_label] = values
  return(labeled_cluster)

data_list = clustering_data.values.tolist()
labeled_cluster = return_labeled_clusters(data_list, labels, complete_link_clusters)
labeled_cluster

train_knn_classifier = []
for label, values in labeled_cluster.items():
  for features in values:
    feature_label = [features, label]
    train_knn_classifier.append(feature_label)
train_knn_classifier

def K_near_class_pred(training_data, testing_data, k):
  traind = training_data
  testd = testing_data
  distance = []
  dist_and_class= []
  for j in range(len(traind)):
    label_and_distance_list = []
    a = traind[j][0]
    sum = 0
    for i in range(len(a)):
      sum += ((a[i] - testd[i])**2)
    cartesian_dist = math.sqrt(sum)
    distance.append(cartesian_dist)
    label_and_distance_list.append(cartesian_dist) 
    label_and_distance_list.append(traind[j][1])
    dist_and_class.append(label_and_distance_list)

  def K_nearest_neighbor(k, distance, data_class):
    distance = sorted(distance)
    label = []
    for j in range(k):
      for i in range(len(data_class)):
        if distance[j] == data_class[i][0]:
          label.append(data_class[i][1])
    
    return(max(set(label), key = label.count))
  
  pred = K_nearest_neighbor(k, distance, dist_and_class)
  return (pred)

# defining a function to normalize the data
def train_test_data_normalization(data, train_or_test):
  if train_or_test.lower() == 'test':
    test_df = pd.DataFrame(data)
    # creating a description dataframe to get the min and 
    # max value for each column(feature)
    description = test_df.describe().transpose()
    mean_norm = (test_df - description['min'])/(
          description['max'] - description['min'])
    # returning a list of list with normalized data
    data_for_clustering = mean_norm.values.tolist()
    return (data_for_clustering)

  if train_or_test.lower() == 'train':
    label_list = []
    data_point = []
    for i in range(len(data)):
      label_list.append(data[i][1])
      data_point.append(data[i][0])
    train_df = pd.DataFrame(data_point)
    # creating a description dataframe to get the min and 
    # max value for each column(feature)
    description = train_df.describe().transpose()
    mean_norm = (train_df - description['min'])/(
          description['max'] - description['min'])
    # returning a list of list with normalized data
    mean_norm['class'] = label_list
    train_data_list = mean_norm.values.tolist()
    data_for_clustering = []
    for p in range(len(train_data_list)):
      label = train_data_list[p].pop(-1)
      ith_value = [train_data_list[p], label]
      data_for_clustering.append(ith_value)
  return (data_for_clustering)

norm_train_knn = train_test_data_normalization(train_knn_classifier, 'train')
norm_test_knn = train_test_data_normalization(testing_data, 'test')

actual_y = testing_data_actual_labels.values.tolist()
for k in range(1,11):
  pred_y = []
  for i in range(len(norm_test_knn)):
    pred_yi = K_near_class_pred(norm_train_knn, norm_test_knn[i], k = k)
    pred_y.append(pred_yi)

  score = 0
  for i in range(len(pred_y)):
    if pred_y[i] == actual_y[i]:
      score+=1
  accuracy = score/len(pred_y)
  print('For k =', k, 'the accuracy is :', accuracy)

"""**Splitting the data to training and testing after the clustering:**"""

# Read .txt files

path = '/content/drive/MyDrive/final_project_data/seeds_dataset.txt'
# reading the .xls file into a dataframe
seed_data = pd.read_csv(path, sep = '\s{1,}', 
                              engine = 'python', header=None)

labels = seed_data[seed_data.columns[-1]]
del seed_data[seed_data.columns[-1]]

complete_link_clusters, complete_link_dendogram = hierarchical_clustering(seed_data, 
                        linkage = 'single', 
                        n_clusters = 5)

cluster_list = []
label_cid = []
for label, values in complete_link_clusters.items():
  for features in values:
    cluster_list.append(features)
    label_cid.append(label)
cluster_df = pd.DataFrame(cluster_list)
cluster_df['cluster ID'] = label_cid

# taking the random 10 rows of each class labels as testing data
ten_label_0_data = cluster_df.loc[(cluster_df['cluster ID']== 0)].sample(1, random_state=1)
ten_label_1_data = cluster_df.loc[(cluster_df['cluster ID']== 1)].sample(1, random_state=1)
ten_label_2_data = cluster_df.loc[(cluster_df['cluster ID']== 2)].sample(1, random_state=1)
ten_label_3_data = cluster_df.loc[(cluster_df['cluster ID']== 3)].sample(1, random_state=1)
ten_label_4_data = cluster_df.loc[(cluster_df['cluster ID']== 4)].sample(1, random_state=1)

testing_df = pd.concat([ten_label_0_data, ten_label_1_data , ten_label_2_data#]) 
                        ,ten_label_3_data, ten_label_4_data])

# creating a list of list from testing dataframe and class labels of each data point
testing_data_actual_labels = testing_df[testing_df.columns[-1]]
del testing_df[testing_df.columns[-1]]
testing_data = testing_df.values.tolist()

# Taking the remaining data points as the data for clustering
training_data = cluster_df.drop(testing_df.index).reset_index(drop = True)

training_list = training_data.values.tolist()
train_knn = []
for i in range(len(training_list)):
  label = training_list[i].pop(-1)
  train_knn.append([training_list[i],label])

actual_y = testing_data_actual_labels.values.tolist()
for k in range(1,20):
  pred_y = []
  for i in range(len(testing_data)):
    pred_yi = K_near_class_pred(train_knn, testing_data[i], k = k)
    pred_y.append(pred_yi)

  score = 0
  for i in range(len(pred_y)):
    if pred_y[i] == actual_y[i]:
      score+=1
  accuracy = score/len(pred_y)
  print('For k =', k, 'the accuracy is :', accuracy)

complete_link_clusters, complete_link_dendogram = hierarchical_clustering(seed_data, 
                        linkage = 'complete', 
                        n_clusters = 3)

cluster_list = []
label_cid = []
for label, values in complete_link_clusters.items():
  for features in values:
    cluster_list.append(features)
    label_cid.append(label)
cluster_df = pd.DataFrame(cluster_list)
cluster_df['cluster ID'] = label_cid

# taking the random 10 rows of each class labels as testing data
ten_label_0_data = cluster_df.loc[(cluster_df['cluster ID']== 0)].sample(10, random_state=1)
ten_label_1_data = cluster_df.loc[(cluster_df['cluster ID']== 1)].sample(10, random_state=1)
ten_label_2_data = cluster_df.loc[(cluster_df['cluster ID']== 2)].sample(10, random_state=1)
#ten_label_3_data = cluster_df.loc[(cluster_df['cluster ID']== 3)].sample(1, random_state=1)
#ten_label_4_data = cluster_df.loc[(cluster_df['cluster ID']== 4)].sample(1, random_state=1)

testing_df = pd.concat([ten_label_0_data, ten_label_1_data , ten_label_2_data]) 
                        #,ten_label_3_data, ten_label_4_data])

# creating a list of list from testing dataframe and class labels of each data point
testing_data_actual_labels = testing_df[testing_df.columns[-1]]
del testing_df[testing_df.columns[-1]]
testing_data = testing_df.values.tolist()

# Taking the remaining data points as the data for clustering
training_data = cluster_df.drop(testing_df.index).reset_index(drop = True)

training_list = training_data.values.tolist()
train_knn = []
for i in range(len(training_list)):
  label = training_list[i].pop(-1)
  train_knn.append([training_list[i],label])

actual_y = testing_data_actual_labels.values.tolist()
for k in range(1,20):
  pred_y = []
  for i in range(len(testing_data)):
    pred_yi = K_near_class_pred(train_knn, testing_data[i], k = k)
    pred_y.append(pred_yi)

  score = 0
  for i in range(len(pred_y)):
    if pred_y[i] == actual_y[i]:
      score+=1
  accuracy = score/len(pred_y)
  print('For k =', k, 'the accuracy is :', accuracy)

"""**Testing to see how my KNN algorithm behaves on a 3 dimensional feature space**

**Visualizing the first Approach: Splitting the test set before clustering**
"""

from sklearn import datasets

iris = datasets.load_iris()

iris_df= pd.DataFrame(iris.data[:, :3])
iris_df['class'] = iris.target
iris_df

from mpl_toolkits.mplot3d import Axes3D
import plotly.express as px
fig = px.scatter_3d(iris_df, x=0, y=1, z=2,
                    color = 'class', title = 'Original Dataset')
fig.show()

# getting the name of the last column that has class label
last_column = list(iris_df.columns)[-1]

# taking the random 10 rows of each class labels as testing data
ten_label_0_data = iris_df.loc[(iris_df[last_column]== 0)].sample(10, random_state=1)
ten_label_1_data = iris_df.loc[(iris_df[last_column]== 1)].sample(10, random_state=1)
ten_label_2_data = iris_df.loc[(iris_df[last_column]== 2)].sample(10, random_state=1)
testing_df = pd.concat([ten_label_0_data, ten_label_1_data , ten_label_2_data ])

# creating a list of list from testing dataframe and class labels of each data point
testing_data_actual_labels = testing_df[testing_df.columns[-1]]
del testing_df[testing_df.columns[-1]]
testing_data = testing_df.values.tolist()

# Taking the remaining data points as the data for clustering
clustering_data = iris_df.drop(testing_df.index).reset_index(drop = True)

# dropping the last column of the dataframe.
# this column contains the class of the sample
labels = clustering_data[clustering_data.columns[-1]]
del clustering_data[clustering_data.columns[-1]]

iris_clustering = hierarchical_clustering(clustering_data, 'single', 10)

cluster_dictionary = iris_clustering[0]
data = clustering_data.values.tolist()
labeled_cluster = return_labeled_clusters(data, labels, cluster_dictionary)
cluster = []
cluster_class = []
for key, values in labeled_cluster.items():
  for data in values:
    cluster.append(data)
    cluster_class.append(key)
cluster_df = pd.DataFrame(cluster)
cluster_df['class'] = cluster_class


fig = px.scatter_3d(cluster_df, x=0, y=1, z=2,
                    color = 'class', title = 'Our data after clustering on single linkage')
fig.show()

iris_clustering = hierarchical_clustering(clustering_data, 'complete', 10)

cluster_dictionary = iris_clustering[0]
data = clustering_data.values.tolist()
labeled_cluster = return_labeled_clusters(data, labels, cluster_dictionary)
cluster = []
cluster_class = []
for key, values in labeled_cluster.items():
  for data in values:
    cluster.append(data)
    cluster_class.append(key)
cluster_df = pd.DataFrame(cluster)
cluster_df['class'] = cluster_class


fig = px.scatter_3d(cluster_df, x=0, y=1, z=2,
                    color = 'class', title = 'Our data after clustering on complete linkage')
fig.show()

# Plotting test data
from mpl_toolkits.mplot3d import Axes3D
import plotly.express as px
fig = px.scatter_3d(pd.DataFrame(testing_data), x=0, y=1, z=2, title = 'Our Test data')
fig.show()

train_knn_classifier = []
for label, values in labeled_cluster.items():
  for features in values:
    feature_label = [features, label]
    train_knn_classifier.append(feature_label)
train_knn_classifier

for k in range(1,11):
  y_pred = []
  for i in range(len(testing_data)):
    pred = K_near_class_pred(train_knn_classifier, testing_data[i], k)
    y_pred.append(pred)
  test_df = pd.DataFrame(testing_data)
  test_df['pred class'] = y_pred

  # Plotting test data with predicted class
  ttle = 'When k =' + str(k)
  fig = px.scatter_3d(test_df, x=0, y=1, z=2, color = 'pred class', 
                      title = ttle)
  fig.show()

"""**Visualizing the second Approach: Splitting the test set after clustering**"""

iris = datasets.load_iris()

iris_df= pd.DataFrame(iris.data[:, :3])
iris_df['class'] = iris.target
iris_df

complete_link_clusters, complete_link_dendogram = hierarchical_clustering(iris_df, 
                        linkage = 'complete', 
                        n_clusters = 3)

cluster_list = []
label_cid = []
for label, values in complete_link_clusters.items():
  for features in values:
    cluster_list.append(features)
    label_cid.append(label)
cluster_df = pd.DataFrame(cluster_list)
cluster_df['cluster ID'] = label_cid

# plotting to visualize the clusters
fig = px.scatter_3d(cluster_df, x=0, y=1, z=2, color = 'cluster ID', 
                    title = 'Our clusters')
fig.show()

# taking the random 10 rows of each class labels as testing data
ten_label_0_data = cluster_df.loc[(cluster_df['cluster ID']== 0)].sample(10, random_state=1)
ten_label_1_data = cluster_df.loc[(cluster_df['cluster ID']== 1)].sample(10, random_state=1)
ten_label_2_data = cluster_df.loc[(cluster_df['cluster ID']== 2)].sample(10, random_state=1)
#ten_label_3_data = cluster_df.loc[(cluster_df['cluster ID']== 3)].sample(10, random_state=1)
#ten_label_4_data = cluster_df.loc[(cluster_df['cluster ID']== 4)].sample(10, random_state=1)

testing_df = pd.concat([ten_label_0_data, ten_label_1_data , ten_label_2_data]) 
                        #,ten_label_3_data, ten_label_4_data])

# creating a list of list from testing dataframe and class labels of each data point
testing_data_actual_labels = testing_df[testing_df.columns[-1]]
del testing_df[testing_df.columns[-1]]
testing_data = testing_df.values.tolist()

# plotting to visualize the test data
fig = px.scatter_3d(pd.DataFrame(testing_data), x=0, y=1, z=2, title = 'Our Test data')
fig.show()

# Taking the remaining data points as the data for clustering
training_data = cluster_df.drop(testing_df.index).reset_index(drop = True)

training_list = training_data.values.tolist()
train_knn = []
for i in range(len(training_list)):
  label = training_list[i].pop(-1)
  train_knn.append([training_list[i],label])

for k in range(1,11):
  pred_y = []
  for i in range(len(testing_data)):
    pred_yi = K_near_class_pred(train_knn, testing_data[i], k = k)
    pred_y.append(pred_yi)

  testing_data_with_pred_labels = pd.DataFrame(testing_data)
  testing_data_with_pred_labels['pred cluster id'] = pred_y

  tle = "Predicted cluster ID When k = " + str(k)
  fig = px.scatter_3d(testing_data_with_pred_labels, x=0, y=1, z=2, 
                      color = 'pred cluster id', title = tle)
  fig.show()